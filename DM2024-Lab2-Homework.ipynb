{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tckObZEkV9VH"
   },
   "source": [
    "### Student Information\n",
    "Name: 張子凡\n",
    "\n",
    "Student ID: 112139503   \n",
    "\n",
    "GitHub ID: spikey09\n",
    "\n",
    "Kaggle name: spikeyscii\n",
    "\n",
    "Kaggle private scoreboard snapshot:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L3XZ2AM3V9VI"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tHCChtDXV9VJ"
   },
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RvrnRS4BV9VJ"
   },
   "source": [
    "1. First: __This part is worth 30% of your grade.__ Do the **take home exercises** in the [DM2024-Lab2-master Repo](https://github.com/didiersalazar/DM2024-Lab2-Master). You may need to copy some cells from the Lab notebook to this notebook.\n",
    "\n",
    "\n",
    "2. Second: __This part is worth 30% of your grade.__ Participate in the in-class [Kaggle Competition](https://www.kaggle.com/competitions/dm-2024-isa-5810-lab-2-homework) regarding Emotion Recognition on Twitter by this link: https://www.kaggle.com/competitions/dm-2024-isa-5810-lab-2-homework. The scoring will be given according to your place in the Private Leaderboard ranking:\n",
    "    - **Bottom 40%**: Get 20% of the 30% available for this section.\n",
    "\n",
    "    - **Top 41% - 100%**: Get (0.6N + 1 - x) / (0.6N) * 10 + 20 points, where N is the total number of participants, and x is your rank. (ie. If there are 100 participants and you rank 3rd your score will be (0.6 * 100 + 1 - 3) / (0.6 * 100) * 10 + 20 = 29.67% out of 30%.)   \n",
    "    Submit your last submission **BEFORE the deadline (Nov. 26th, 11:59 pm, Tuesday)**. Make sure to take a screenshot of your position at the end of the competition and store it as '''pic0.png''' under the **img** folder of this repository and rerun the cell **Student Information**.\n",
    "    \n",
    "\n",
    "3. Third: __This part is worth 30% of your grade.__ A report of your work developing the model for the competition (You can use code and comment on it). This report should include what your preprocessing steps, the feature engineering steps and an explanation of your model. You can also mention different things you tried and insights you gained.\n",
    "\n",
    "\n",
    "4. Fourth: __This part is worth 10% of your grade.__ It's hard for us to follow if your code is messy :'(, so please **tidy up your notebook**.\n",
    "\n",
    "\n",
    "Upload your files to your repository then submit the link to it on the corresponding e-learn assignment.\n",
    "\n",
    "Make sure to commit and save your changes to your repository __BEFORE the deadline (Nov. 26th, 11:59 pm, Tuesday)__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install package\n",
    "Automatically check and install package for quckily deploy on any local device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas is already installed.\n",
      "numpy is already installed.\n",
      "nltk is already installed.\n",
      "matplotlib is already installed.\n",
      "seaborn is already installed.\n",
      "umap-learn is not installed. Installing...\n",
      "gensim is already installed.\n",
      "tensorflow is already installed.\n",
      "keras is already installed.\n",
      "ollama is already installed.\n",
      "langchain is already installed.\n",
      "langchain_community is already installed.\n",
      "langchain_core is already installed.\n",
      "bs4 is already installed.\n",
      "chromadb is already installed.\n",
      "gradio is already installed.\n",
      "emoji is already installed.\n",
      "All libraries checked.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# 函式清單，包含需要檢查的函式庫名稱（可以指定版本）\n",
    "libraries = {\n",
    "    \"pandas\": None,  # 最新版本\n",
    "    \"numpy\": None,\n",
    "    \"nltk\": None,\n",
    "    \"matplotlib\": None,\n",
    "    \"seaborn\": None,\n",
    "    \"itertools\": None,  # itertools 是內建模組，無需安裝\n",
    "    \"umap-learn\": None,\n",
    "    \"gensim\": None,\n",
    "    \"tensorflow\": None,\n",
    "    \"keras\": None,\n",
    "    \"ollama\": None,\n",
    "    \"langchain\": None,\n",
    "    \"langchain_community\": None,\n",
    "    \"langchain_core\": None,\n",
    "    \"bs4\": None,\n",
    "    \"chromadb\": None,\n",
    "    \"gradio\": None,\n",
    "    \"emoji\": None  # 指定版本\n",
    "}\n",
    "\n",
    "# 檢查函式庫是否已安裝，若未安裝則自動安裝\n",
    "for lib, version in libraries.items():\n",
    "    try:\n",
    "        if lib == \"itertools\":\n",
    "            # itertools 是內建模組，直接跳過\n",
    "            continue\n",
    "        \n",
    "        # 若有指定版本，檢查該版本是否已安裝\n",
    "        if version:\n",
    "            import pkg_resources\n",
    "            pkg_resources.require(f\"{lib}=={version}\")\n",
    "            print(f\"{lib}=={version} is already installed.\")\n",
    "        else:\n",
    "            # 若無版本限制，只檢查模組是否存在\n",
    "            __import__(lib)\n",
    "            print(f\"{lib} is already installed.\")\n",
    "    except ImportError:\n",
    "        # 安裝指定版本或最新版本\n",
    "        print(f\"{lib} is not installed. Installing...\")\n",
    "        try:\n",
    "            if version:\n",
    "                subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", f\"{lib}=={version}\"])\n",
    "            else:\n",
    "                subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", lib])\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to install {lib}: {e}\")\n",
    "    except pkg_resources.VersionConflict as e:\n",
    "        # 如果版本不符，重新安裝指定版本\n",
    "        print(f\"Version conflict for {lib}. Reinstalling version {version}...\")\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", f\"{lib}=={version}\"])\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to install {lib}: {e}\")\n",
    "\n",
    "print(\"All libraries checked.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Nov 25 20:04:02 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.94                 Driver Version: 560.94         CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4080 ...  WDDM  |   00000000:01:00.0  On |                  N/A |\n",
      "|  0%   47C    P8             13W /  320W |    8051MiB /  16376MiB |      7%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      2132    C+G   ...oogle\\Chrome\\Application\\chrome.exe      N/A      |\n",
      "|    0   N/A  N/A      2480    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A      4948    C+G   ...US\\ArmouryDevice\\asus_framework.exe      N/A      |\n",
      "|    0   N/A  N/A      7340    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
      "|    0   N/A  N/A      9700    C+G   ...crosoft\\Edge\\Application\\msedge.exe      N/A      |\n",
      "|    0   N/A  N/A     11284    C+G   ...t.LockApp_cw5n1h2txyewy\\LockApp.exe      N/A      |\n",
      "|    0   N/A  N/A     14016    C+G   ...siveControlPanel\\SystemSettings.exe      N/A      |\n",
      "|    0   N/A  N/A     17532      C   ...er\\anaconda3\\envs\\env_dm\\python.exe      N/A      |\n",
      "|    0   N/A  N/A     18320    C+G   ...oogle\\Chrome\\Application\\chrome.exe      N/A      |\n",
      "|    0   N/A  N/A     19044    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe      N/A      |\n",
      "|    0   N/A  N/A     19196    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     19520    C+G   ...Programs\\Microsoft VS Code\\Code.exe      N/A      |\n",
      "|    0   N/A  N/A     21292    C+G   ...x64__qmba6cd70vzyy\\ArmouryCrate.exe      N/A      |\n",
      "|    0   N/A  N/A     21768    C+G   C:\\Windows\\explorer.exe                     N/A      |\n",
      "|    0   N/A  N/A     22292    C+G   ...cal\\Microsoft\\OneDrive\\OneDrive.exe      N/A      |\n",
      "|    0   N/A  N/A     24276    C+G   ...2txyewy\\StartMenuExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     29272    C+G   ...on\\131.0.2903.63\\msedgewebview2.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LxGoIMMtfpcG"
   },
   "source": [
    "## Data\n",
    "Loading data emmotion and corpus marge to training and testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "4mkqPsQBWShQ",
    "outputId": "c539e988-acb0-4395-83f6-c591bb7e2594"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>identification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x28cc61</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x29e452</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x2b3819</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x2db41f</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2a2acc</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id identification\n",
       "0  0x28cc61           test\n",
       "1  0x29e452          train\n",
       "2  0x2b3819          train\n",
       "3  0x2db41f           test\n",
       "4  0x2a2acc          train"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_path = 'data'\n",
    "tweet_id = pd.read_csv(f'{input_path}/data_identification.csv')\n",
    "tweet_id.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "2z5V1Xy5XdQK",
    "outputId": "8bf5177d-6aad-4e6f-c689-febe2b8f5f8b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x29e452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x2b3819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2a2acc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0x2a8830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0x20b21d</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id\n",
       "1  0x29e452\n",
       "2  0x2b3819\n",
       "4  0x2a2acc\n",
       "5  0x2a8830\n",
       "6  0x20b21d"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tweet_id = tweet_id[tweet_id['identification'] == 'train'].drop(['identification'], axis=1)\n",
    "test_tweet_id = tweet_id[tweet_id['identification'] == 'test'].drop(['identification'], axis=1)\n",
    "\n",
    "train_tweet_id.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "n8DfUCWqWuQi",
    "outputId": "925e477e-c609-42db-a0a0-5488f5cd9d89"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x3140b1</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x368b73</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x296183</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x2bd6e1</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2ee1dd</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id       emotion\n",
       "0  0x3140b1       sadness\n",
       "1  0x368b73       disgust\n",
       "2  0x296183  anticipation\n",
       "3  0x2bd6e1           joy\n",
       "4  0x2ee1dd  anticipation"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_labels = pd.read_csv(f'{input_path}/emotion.csv')\n",
    "emotion_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "id": "e6ik_1LcW_oQ",
    "outputId": "e099647f-ba78-4c96-c52e-2f5ca7d6da98"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_score</th>\n",
       "      <th>_index</th>\n",
       "      <th>_crawldate</th>\n",
       "      <th>_type</th>\n",
       "      <th>tweet.hashtags</th>\n",
       "      <th>tweet.tweet_id</th>\n",
       "      <th>tweet.text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>391</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>2015-05-23 11:42:47</td>\n",
       "      <td>tweets</td>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>433</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>2016-01-28 04:52:09</td>\n",
       "      <td>tweets</td>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>232</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>2017-12-25 04:39:20</td>\n",
       "      <td>tweets</td>\n",
       "      <td>[bibleverse]</td>\n",
       "      <td>0x28b412</td>\n",
       "      <td>Confident of your obedience, I write to you, k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>376</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>2016-01-24 23:53:05</td>\n",
       "      <td>tweets</td>\n",
       "      <td>[]</td>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha 😂😂😂 &lt;LH&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>989</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>2016-01-08 17:18:59</td>\n",
       "      <td>tweets</td>\n",
       "      <td>[]</td>\n",
       "      <td>0x2de201</td>\n",
       "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   _score          _index           _crawldate   _type  \\\n",
       "0     391  hashtag_tweets  2015-05-23 11:42:47  tweets   \n",
       "1     433  hashtag_tweets  2016-01-28 04:52:09  tweets   \n",
       "2     232  hashtag_tweets  2017-12-25 04:39:20  tweets   \n",
       "3     376  hashtag_tweets  2016-01-24 23:53:05  tweets   \n",
       "4     989  hashtag_tweets  2016-01-08 17:18:59  tweets   \n",
       "\n",
       "                  tweet.hashtags tweet.tweet_id  \\\n",
       "0                     [Snapchat]       0x376b20   \n",
       "1  [freepress, TrumpLegacy, CNN]       0x2d5350   \n",
       "2                   [bibleverse]       0x28b412   \n",
       "3                             []       0x1cd5b0   \n",
       "4                             []       0x2de201   \n",
       "\n",
       "                                          tweet.text  \n",
       "0  People who post \"add me on #Snapchat\" must be ...  \n",
       "1  @brianklaas As we see, Trump is dangerous to #...  \n",
       "2  Confident of your obedience, I write to you, k...  \n",
       "3                Now ISSA is stalking Tasha 😂😂😂 <LH>  \n",
       "4  \"Trust is not the same as faith. A friend is s...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df = pd.read_json(f'{input_path}/tweets_DM.json', lines=True)\n",
    "source_df = pd.json_normalize(tweets_df['_source'])\n",
    "tweets_df = pd.concat([tweets_df.drop(columns=['_source']), source_df], axis=1)\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "id": "LSToyFxIW2io",
    "outputId": "a0fb7772-3f8d-4217-b934-8929642589e9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>emotion</th>\n",
       "      <th>_score</th>\n",
       "      <th>_index</th>\n",
       "      <th>_crawldate</th>\n",
       "      <th>_type</th>\n",
       "      <th>tweet.hashtags</th>\n",
       "      <th>tweet.tweet_id</th>\n",
       "      <th>tweet.text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x29e452</td>\n",
       "      <td>joy</td>\n",
       "      <td>809</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>2015-01-17 03:07:03</td>\n",
       "      <td>tweets</td>\n",
       "      <td>[]</td>\n",
       "      <td>0x29e452</td>\n",
       "      <td>Huge Respect🖒 @JohnnyVegasReal talking about l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2b3819</td>\n",
       "      <td>joy</td>\n",
       "      <td>808</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>2016-07-02 09:34:06</td>\n",
       "      <td>tweets</td>\n",
       "      <td>[spateradio, app]</td>\n",
       "      <td>0x2b3819</td>\n",
       "      <td>Yoooo we hit all our monthly goals with the ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x2a2acc</td>\n",
       "      <td>trust</td>\n",
       "      <td>16</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>2016-08-15 18:18:39</td>\n",
       "      <td>tweets</td>\n",
       "      <td>[]</td>\n",
       "      <td>0x2a2acc</td>\n",
       "      <td>@KIDSNTS @PICU_BCH @uhbcomms @BWCHBoss Well do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x2a8830</td>\n",
       "      <td>joy</td>\n",
       "      <td>768</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>2017-02-11 08:49:46</td>\n",
       "      <td>tweets</td>\n",
       "      <td>[PUBG, GamersUnite, twitch, BeHealthy, StayPos...</td>\n",
       "      <td>0x2a8830</td>\n",
       "      <td>Come join @ambushman27 on #PUBG while he striv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x20b21d</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>70</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>2016-11-23 05:37:10</td>\n",
       "      <td>tweets</td>\n",
       "      <td>[strength, bones, God]</td>\n",
       "      <td>0x20b21d</td>\n",
       "      <td>@fanshixieen2014 Blessings!My #strength little...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id       emotion  _score          _index           _crawldate  \\\n",
       "0  0x29e452           joy     809  hashtag_tweets  2015-01-17 03:07:03   \n",
       "1  0x2b3819           joy     808  hashtag_tweets  2016-07-02 09:34:06   \n",
       "2  0x2a2acc         trust      16  hashtag_tweets  2016-08-15 18:18:39   \n",
       "3  0x2a8830           joy     768  hashtag_tweets  2017-02-11 08:49:46   \n",
       "4  0x20b21d  anticipation      70  hashtag_tweets  2016-11-23 05:37:10   \n",
       "\n",
       "    _type                                     tweet.hashtags tweet.tweet_id  \\\n",
       "0  tweets                                                 []       0x29e452   \n",
       "1  tweets                                  [spateradio, app]       0x2b3819   \n",
       "2  tweets                                                 []       0x2a2acc   \n",
       "3  tweets  [PUBG, GamersUnite, twitch, BeHealthy, StayPos...       0x2a8830   \n",
       "4  tweets                             [strength, bones, God]       0x20b21d   \n",
       "\n",
       "                                          tweet.text  \n",
       "0  Huge Respect🖒 @JohnnyVegasReal talking about l...  \n",
       "1  Yoooo we hit all our monthly goals with the ne...  \n",
       "2  @KIDSNTS @PICU_BCH @uhbcomms @BWCHBoss Well do...  \n",
       "3  Come join @ambushman27 on #PUBG while he striv...  \n",
       "4  @fanshixieen2014 Blessings!My #strength little...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge enmotion attribute\n",
    "train_data = pd.merge(train_tweet_id, emotion_labels, on='tweet_id', how='inner')\n",
    "train_data = pd.merge(train_data, tweets_df, left_on='tweet_id', right_on='tweet.tweet_id', how='inner')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "id": "tQ3sHUB9cV-N",
    "outputId": "013ea68b-0132-4719-d017-808fc1d822fe"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>_score</th>\n",
       "      <th>_index</th>\n",
       "      <th>_crawldate</th>\n",
       "      <th>_type</th>\n",
       "      <th>tweet.hashtags</th>\n",
       "      <th>tweet.tweet_id</th>\n",
       "      <th>tweet.text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x28cc61</td>\n",
       "      <td>107</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>2017-01-17 14:13:32</td>\n",
       "      <td>tweets</td>\n",
       "      <td>[]</td>\n",
       "      <td>0x28cc61</td>\n",
       "      <td>@Habbo I've seen two separate colours of the e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2db41f</td>\n",
       "      <td>728</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>2015-10-17 06:46:20</td>\n",
       "      <td>tweets</td>\n",
       "      <td>[]</td>\n",
       "      <td>0x2db41f</td>\n",
       "      <td>@FoxNews @KellyannePolls No serious self respe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x2466f6</td>\n",
       "      <td>491</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>2016-12-19 03:50:27</td>\n",
       "      <td>tweets</td>\n",
       "      <td>[womendrivers]</td>\n",
       "      <td>0x2466f6</td>\n",
       "      <td>Looking for a new car, and it says 1 lady owne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x23f9e9</td>\n",
       "      <td>28</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>2017-04-09 19:32:19</td>\n",
       "      <td>tweets</td>\n",
       "      <td>[robbingmembers]</td>\n",
       "      <td>0x23f9e9</td>\n",
       "      <td>@cineworld “only the brave” just out and fount...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x1fb4e1</td>\n",
       "      <td>925</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>2016-01-15 11:59:31</td>\n",
       "      <td>tweets</td>\n",
       "      <td>[]</td>\n",
       "      <td>0x1fb4e1</td>\n",
       "      <td>Felt like total dog 💩 going into open gym and ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id  _score          _index           _crawldate   _type  \\\n",
       "0  0x28cc61     107  hashtag_tweets  2017-01-17 14:13:32  tweets   \n",
       "1  0x2db41f     728  hashtag_tweets  2015-10-17 06:46:20  tweets   \n",
       "2  0x2466f6     491  hashtag_tweets  2016-12-19 03:50:27  tweets   \n",
       "3  0x23f9e9      28  hashtag_tweets  2017-04-09 19:32:19  tweets   \n",
       "4  0x1fb4e1     925  hashtag_tweets  2016-01-15 11:59:31  tweets   \n",
       "\n",
       "     tweet.hashtags tweet.tweet_id  \\\n",
       "0                []       0x28cc61   \n",
       "1                []       0x2db41f   \n",
       "2    [womendrivers]       0x2466f6   \n",
       "3  [robbingmembers]       0x23f9e9   \n",
       "4                []       0x1fb4e1   \n",
       "\n",
       "                                          tweet.text  \n",
       "0  @Habbo I've seen two separate colours of the e...  \n",
       "1  @FoxNews @KellyannePolls No serious self respe...  \n",
       "2  Looking for a new car, and it says 1 lady owne...  \n",
       "3  @cineworld “only the brave” just out and fount...  \n",
       "4  Felt like total dog 💩 going into open gym and ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.merge(test_tweet_id, tweets_df, left_on='tweet_id', right_on='tweet.tweet_id', how='inner')\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9A9UDP_AfodN"
   },
   "source": [
    "## Data preprocessing\n",
    "Preparing data for feeding into a model by, cleaning, transforming, and reduces noise in dataset. While the step are critical for ensuring data quility, their impact on model performance might be limited if there is unsufficient understanding of the specific model's requirement and its sensitivity to different types of feature. Maybe there tweet courpus is shot and clear enough, doesn't need additional pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # 移除 <LH> 標籤\n",
    "    text = text.replace('<LH>', '')\n",
    "    \n",
    "    # # 移除網址\n",
    "    # text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    \n",
    "    # # 移除提及標記 (@用戶名)\n",
    "    # text = re.sub(r'@\\w+', '', text)\n",
    "    \n",
    "    # # 移除特殊字符或多餘的符號\n",
    "    # text = re.sub(r'[^A-Za-z0-9\\s.,!?]', '', text)\n",
    "    \n",
    "    # 壓縮多餘的空格\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply preprocessing function to the text column\n",
    "train_data['processed_text'] = train_data['tweet.text'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Huge Respect🖒 @JohnnyVegasReal talking about l...\n",
       "1    Yoooo we hit all our monthly goals with the ne...\n",
       "2    @KIDSNTS @PICU_BCH @uhbcomms @BWCHBoss Well do...\n",
       "3    Come join @ambushman27 on #PUBG while he striv...\n",
       "4    @fanshixieen2014 Blessings!My #strength little...\n",
       "Name: tweet.text, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['tweet.text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Huge Respect🖒 @JohnnyVegasReal talking about l...\n",
       "1    Yoooo we hit all our monthly goals with the ne...\n",
       "2    @KIDSNTS @PICU_BCH @uhbcomms @BWCHBoss Well do...\n",
       "3    Come join @ambushman27 on #PUBG while he striv...\n",
       "4    @fanshixieen2014 Blessings!My #strength little...\n",
       "Name: processed_text, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['processed_text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Ed_hrEVqin1i"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = train_data['processed_text']\n",
    "y = train_data['emotion']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XFO4QY2ufzzZ"
   },
   "source": [
    "## Model training\n",
    "This process involve tokenizing text into token IDs and attention mask, encoding lables, and batching using **data loaders**. The BERTweet is a large-scale pre-trained language model for English Tweets, the paper shows that the model producing better performance result on three Tweet NLP tasks: part-of-speech tagging, named-entity recognition and text classification, compare agaings previous state-of-the-art model RoBERTa-base and XLM-R-base with strong baseline. So, is reasonable to use BERTweet as the model for Tweets sentiment classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import BertweetTokenizer, AutoModelForSequenceClassification\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(X_train, X_val, y_train, y_val, tokenizer, max_length=128):\n",
    "    # Tokenize text data\n",
    "    train_encodings = tokenizer(list(X_train), truncation=True, padding=True, max_length=max_length, return_tensors=\"pt\")\n",
    "    val_encodings = tokenizer(list(X_val), truncation=True, padding=True, max_length=max_length, return_tensors=\"pt\")\n",
    "\n",
    "    # Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    train_labels = label_encoder.fit_transform(y_train)\n",
    "    val_labels = label_encoder.transform(y_val)\n",
    "\n",
    "    return train_encodings, val_encodings, train_labels, val_labels, label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "loUP-827g_rt"
   },
   "outputs": [],
   "source": [
    "# Dataset class for loading text data from DataFrame\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings  # Should already contain tensors if return_tensors=\"pt\" was used\n",
    "        self.labels = torch.tensor(labels)  # Ensure labels are converted to a tensor\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Use encodings as they are without re-wrapping in torch.tensor\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def train_model(model, optimizer, scheduler, train_dataloader, val_dataloader, device, num_epochs, label_encoder, save_dir=\"./models\"):\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "\n",
    "        # Training phase\n",
    "        for batch in tqdm(train_dataloader, desc=\"Training\"):\n",
    "            b_input_ids = batch['input_ids'].to(device)\n",
    "            b_attention_mask = batch['attention_mask'].to(device)\n",
    "            b_labels = batch['labels'].to(device).long()\n",
    "            \n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=b_input_ids, attention_mask=b_attention_mask, labels=b_labels)\n",
    "            loss = outputs.loss\n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Clip the gradient to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "\n",
    "        # Step the scheduler after each epoch\n",
    "        scheduler.step()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Training Loss: {avg_train_loss}\")\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_dataloader, desc=\"Validating\"):\n",
    "                b_input_ids = batch['input_ids'].to(device)\n",
    "                b_attention_mask = batch['attention_mask'].to(device)\n",
    "                b_labels = batch['labels'].to(device).long()\n",
    "                \n",
    "                # Forward pass for validation\n",
    "                outputs = model(input_ids=b_input_ids, attention_mask=b_attention_mask, labels=b_labels)\n",
    "                loss = outputs.loss\n",
    "                logits = outputs.logits\n",
    "                \n",
    "                total_val_loss += loss.item()\n",
    "                \n",
    "                # Store predictions and true labels\n",
    "                _, preds = torch.max(logits, dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(b_labels.cpu().numpy())\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "        \n",
    "        # Decode numerical labels to original class names\n",
    "        decoded_preds = label_encoder.inverse_transform(all_preds)\n",
    "        decoded_labels = label_encoder.inverse_transform(all_labels)\n",
    "        \n",
    "        # Generate classification report\n",
    "        class_report = classification_report(decoded_labels, decoded_preds, digits=4, target_names=label_encoder.classes_)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Validation Loss: {avg_val_loss}\")\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(class_report)\n",
    "\n",
    "        # Save model checkpoint\n",
    "        save_path = f\"{save_dir}/ep_{epoch + 1}\"\n",
    "        model.save_pretrained(save_path)\n",
    "        print(f\"Model checkpoint saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"vinai/bertweet-base\"\n",
    "tokenizer = BertweetTokenizer.from_pretrained(model_name, normalization=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=8).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_encoding, X_val_encoding, y_train_label, y_val_label, label_encoder = encode_data(X_train, X_val, y_train, y_val, tokenizer, max_length=48)\n",
    "\n",
    "# Initialize datasets\n",
    "train_dataset = TweetDataset(X_train_encoding, y_train_label)\n",
    "val_dataset = TweetDataset(X_val_encoding, y_val_label)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 18195/18195 [27:19<00:00, 11.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Training Loss: 1.0013337626336662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 4549/4549 [01:54<00:00, 39.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Validation Loss: 0.918088096488987\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger     0.5513    0.4216    0.4778      7964\n",
      "anticipation     0.7191    0.7447    0.7317     49725\n",
      "     disgust     0.5363    0.5633    0.5495     27892\n",
      "        fear     0.7345    0.5895    0.6541     12955\n",
      "         joy     0.7265    0.8062    0.7643    103089\n",
      "     sadness     0.5301    0.6925    0.6005     38835\n",
      "    surprise     0.6945    0.3253    0.4431      9750\n",
      "       trust     0.7473    0.4410    0.5547     40903\n",
      "\n",
      "    accuracy                         0.6697    291113\n",
      "   macro avg     0.6550    0.5730    0.5970    291113\n",
      "weighted avg     0.6782    0.6697    0.6633    291113\n",
      "\n",
      "Model checkpoint saved to ./models/Bertweet_v3/ep_1\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "scheduler = StepLR(optimizer, step_size=2, gamma=0.7)\n",
    "train_model(model, optimizer, scheduler, train_dataloader, val_dataloader, device, num_epochs=1, label_encoder=label_encoder, save_dir=\"./models/Bertweet_v3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INFERENCE\n",
    "Sentiment classification task involves tokenizing the input text using the BERTweet tokenizer, converting it into token IDs and attention masks, and passing these inputs into the pre-trained BERTweet model. During the forward pass, the token IDs are transformed into dense embeddings by the model's embedding layer, which are then processed through the transformer layers to generate logits representing class probabilities and futher inference emotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertweetTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['processed_text'] = test_data['tweet.text'].apply(preprocess_text)\n",
    "X_test = test_data['processed_text']\n",
    "\n",
    "# Load the tokenizer and model\n",
    "model_name = \"vinai/bertweet-base\"\n",
    "tokenizer = BertweetTokenizer.from_pretrained(model_name, normalization=True)\n",
    "test_encodings = tokenizer(list(X_test), truncation=True, padding=True, max_length=48, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataLoader for batching\n",
    "test_dataset = torch.utils.data.TensorDataset(\n",
    "    test_encodings[\"input_ids\"],\n",
    "    test_encodings[\"attention_mask\"]\n",
    ")\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset,batch_size=512)\n",
    "\n",
    "model_path = \"./models/Bertweet/\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 805/805 [02:29<00:00,  5.37it/s]\n"
     ]
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Perform predictions\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_dataloader, desc=\"Testing\"):\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "\n",
    "        # Get logits from the model\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Get the predicted labels\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        predictions.extend(preds.cpu().numpy())\n",
    "\n",
    "# Map numerical predictions to text labels\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)  # Ensure this matches your training labels\n",
    "predicted_labels = label_encoder.inverse_transform(predictions)\n",
    "\n",
    "# Add predictions to the test DataFrame\n",
    "test_data['emotion'] = predicted_labels  \n",
    "\n",
    "# Keep only the desired columns\n",
    "result = test_data[['tweet_id', 'emotion']]\n",
    "\n",
    "result = result.rename(columns={'tweet_id': 'id'})\n",
    "result.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "1.https://arxiv.org/abs/2005.10200\n",
    "2.https://huggingface.co/docs/transformers/model_doc/bertweet\n",
    "3.https://www.kaggle.com/code/gauravgupta9158/twitter-sentiment-analysis\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "env_dm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
